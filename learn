# Reranking learner

# Anshul Sharma (ansharma)

#!/usr/bin/env python
import itertools
import math
import optparse
import random
import sys
import bleu
from collections import namedtuple

# Modified code for improving the BLEU score of reranked systems. Tuned 
# Perceptron learning parameters, to improve overall accuracy. Decreased
# the learning rate to accomodate for minor changes in classification. 
# Also, decreased the sampler cut off. Added a new feature which computes
# the most common bigrams found over all candidate systems for a particular 
# source sentence. Once all the counts are calculated each sentence can be
# scored according to how common its bigrams are. 

translation_candidate = namedtuple("candidate", "features, smoothed_bleu")
optparser = optparse.OptionParser()
optparser.add_option("-r", "--reference", dest="reference", default="data/train.en", help="English reference sentences")
optparser.add_option("-n", "--nbest", dest="nbest", default="data/train.nbest.shuffled", help="N-best lists")
optparser.add_option("-m", "--num-training-sentences", dest="m", default=sys.maxint, type="int", help="Number of training sentences (default=all)")
optparser.add_option("-t", "--tau", dest="tau", default=5000, type="int", help="PRO samples per input sentence (tau, default=5000)")
optparser.add_option("-x", "--xi", dest="xi", default=50, type="int", help="PRO training instances per input sentence (xi, default=50)")
optparser.add_option("-e", "--eta", dest="eta", default=0.05, type="float", help="Perceptron learning rate (eta, default=0.1)")
optparser.add_option("-a", "--alpha", dest="alpha", default=0.03, type="float", help="Sampler acceptance cutoff (alpha, default=0.05)")
optparser.add_option("-i", "--epochs", dest="epochs", default=5, type="int", help="Perceptron epochs (default=10)")
optparser.add_option("-s", "--random-seed", dest="seed", default="0", type="string", help="Random number seed (default='0')")
(opts,_) = optparser.parse_args()

ref = [line.strip().split() for line in open(opts.reference)][:opts.m]

ngrams = {}
num = {}

sys.stderr.write("Creating ngram feature...")
for n, line in enumerate(open(opts.nbest)):
  (l, sentence, _) = line.strip().split("|||")
  l = int(l)

  sen = [x.lower() for x in sentence.split()]
  for i in range(len(sen) - 1):        
    if (l, (sen[i], sen[i+1])) not in ngrams:
      ngrams[(l, (sen[i], sen[i+1]))] = 0
    ngrams[(l, (sen[i], sen[i+1]))] += 1
  
  if l not in num:
    num[l] = 0
  num[l] += 1

  if n % 2000 == 0:
    sys.stderr.write(".")
sys.stderr.write("\n")

sys.stderr.write("Reading N-best lists...")
nbests = [[] for _ in ref]
for n, line in enumerate(open(opts.nbest)):
  (i, sentence, features) = line.strip().split("|||")
  (i, features) = (int(i), [float(h) for h in features.strip().split()])

  sen = [x.lower() for x in sentence.split()]
  prob = 0.0
  for j in range(len(sen) - 1):
    prob += math.log(float(ngrams[(i, (sen[j], sen[j+1]))])/num[i])
  
  features += [prob]

  if i >= len(ref):
    break
  stats = tuple(bleu.bleu_stats(sentence.strip().split(), ref[i]))
  nbests[i].append(translation_candidate(features, bleu.smoothed_bleu(stats)))
  if n % 2000 == 0:
    sys.stderr.write(".")
sys.stderr.write("\n")

w = [0.0 for _ in xrange(len(nbests[0][0].features))]
random.seed(opts.seed)

# PRO perceptron. Mostly follows the notation of Hopkins & May.
for i in xrange(opts.epochs):
  sys.stderr.write("Epoch %d..." % i)
  (observations, errors) = (0, 0.0)
  for nbest in nbests:
    def V():
      for _ in xrange(opts.tau):
        c1 = random.choice(nbest)
        c2 = random.choice(nbest)
        if c1 != c2 and math.fabs(c1.smoothed_bleu - c2.smoothed_bleu) > opts.alpha:
          yield (c1, c2) if c1.smoothed_bleu > c2.smoothed_bleu else (c2, c1)
    for c1, c2 in sorted(V(), key=lambda (c1,c2): c2.smoothed_bleu-c1.smoothed_bleu)[:opts.xi]: # Figure 4 sampling algorithm
      x = [c1j-c2j for c1j,c2j in zip(c1.features, c2.features)]
      if sum([xj*wj for xj,wj in zip(x,w)]) <= 0:
        errors += 1
        w = [(opts.eta*xj)+wj for xj,wj in zip(x,w)]
      observations += 1
      if observations % 2000 == 0:
        sys.stderr.write(".")
  sys.stderr.write("classification error rate: %f (%d observations)\n" % (errors/observations, observations))

print "\n".join([str(weight) for weight in w])
